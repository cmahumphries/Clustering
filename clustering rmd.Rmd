---
title: "Mini Project K Means Clustering"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Exercise 0

Install these packages if you don't have them already:

*install.packages(c("cluster", "rattle", "NbClust"))*

load the data and look at the first few rows

```{r}
data(wine, package="rattle")
head(wine)
```

##  Exercise 1: 

Remove the first column from the data and scale it using the scale() function

```{r}
df <- scale(wine[-1])
head(df)
```

Now we'd like to cluster the data using K-Means. How do we decide how many clusters to use if you don't know that already? We'll try two methods.

Method 1: A plot of the total within-groups sums of squares against the number of clusters in a K-means solution can be helpful. A bend in the graph can suggest the appropriate number of clusters. 

```{r}
wssplot <- function(data, nc=15, seed=1234){
	              wss <- (nrow(data)-1)*sum(apply(data,2,var))
               	      for (i in 2:nc){
		        set.seed(seed)
	                wss[i] <- sum(kmeans(data, centers=i)$withinss)}
	                
		      plot(1:nc, wss, type="b", xlab="Number of Clusters",
	                        ylab="Within groups sum of squares")
	   }

wssplot(df)
```

## Exercise 2:

* How many clusters does this method suggest?

**Based on where the graph curves, this method suggests 3 clusters**

* Why does this method work? What's the intuition behind it?

**This method explains the degree of variance due to the number of clusters. There is a distinct drop in within groups sum of squares when moving from 1 to 3 clusters. After three clusters this decrease drops off meaning there are decreasing gains by adding further clusters, suggesting that a 3-cluster solution may be a good fit to the data**

* Look at the code for wssplot() and figure out how it works

**wssplot is a function that loops through fifteen k values. The first value for wss is assigned the sum of squares for k=1 by canceling out the n-1 term in the sum of the variances. Then the function loops from k=2 to k=15, assigning the within sum of squares from the kmeans$withinss component for each alue of k. The function then creates a plot of the within groups sum of squares.**

Method 2: Use the NbClust library, which runs many experiments and gives a distribution of potential number of clusters.

```{r}
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]),
	          xlab="Numer of Clusters", ylab="Number of Criteria",
		            main="Number of Clusters Chosen by 26 Criteria")
```

## Exercise 3: 

How many clusters does this method suggest?

**This method also suggests three clusters**

## Exercise 4: 

Once you've picked the number of clusters, run k-means using this number of clusters. Output the result of calling kmeans() into a variable fit.km

```{r}
set.seed(1234)
fit.km <- kmeans(df, centers = 3)
```

Now we want to evaluate how well this clustering does.

## Exercise 5: 

using the table() function, show how the clusters in fit.km\$clusters compares to the actual wine types in wine\$Type. 

```{r}
table(wine$Type, fit.km$cluster)
```

Would you consider this a good clustering?

**Yes: this indicates there are only 6 errors in our model**

## Exercise 6:

* Visualize these clusters using  function clusplot() from the cluster library

```{r}
library(cluster)
clusplot(df, fit.km$cluster)
```

* Would you consider this a good clustering?

**Yes, however it is important to note that only 55% of the variance is shown in this plot, as many variables have been reduced to two components to create this graph**

